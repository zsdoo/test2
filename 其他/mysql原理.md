## 缓冲池

```sql
用于缓存索引和数据的内存大小，这个当然是越多越好， 数据读写在内存中非常快， 减少了对磁盘的读写。 当数据提交或满足检查点条件后才一次性将内存数据刷新到磁盘中。然而内存还有操作系统或数据库其他进程使用， 根据经验，推荐设置innodb-buffer-pool-size为服务器总可用内存的75%。 若设置不当， 内存使用可能浪费或者使用过多。 对于繁忙的服务器， buffer pool 将划分为多个实例以提高系统并发性， 减少线程间读写缓存的争用。buffer pool 的大小首先受 innodb_buffer_pool_instances 影响， 当然影响较小。

Innodb_buffer_pool_pages_data
Innodb buffer pool缓存池中包含数据的页的数目，包括脏页。单位是page。
eg：show global status like 'Innodb_buffer_pool_pages_data';

  
Innodb_buffer_pool_pages_total
innodb buffer pool的页总数目。单位是page。
eg：show global status like 'Innodb_buffer_pool_pages_total';

show global status like 'Innodb_page_size'; 单个页的大小，单位字节（16384/1024=16K）

查看@@innodb_buffer_pool_size大小，单位字节
SELECT @@innodb_buffer_pool_size/1024/1024/1024; #字节转为G

在线调整InnoDB缓冲池大小，如果不设置，默认为128M
set global innodb_buffer_pool_size = 4227858432; ##单位字节

计算Innodb_buffer_pool_pages_data/Innodb_buffer_pool_pages_total*100%
当结果 > 95% 则增加 innodb_buffer_pool_size， 建议使用物理内存的 75%
当结果 < 95% 则减少 innodb_buffer_pool_size， 
建议设置大小为： Innodb_buffer_pool_pages_data * Innodb_page_size * 1.05 / (1024*1024*1024)
```





#### 1、概念

##### 1.1 数据库事务：

简单的说事务就是一组原子性的SQL语句。可以将这组语句理解成一个工作单元，要么全部执行要么都不执行。默认MySQL中自动提交时开启的（start transaction）事务的ACID特性如下：

**原子性：**事务中的所有操作要么全部提交成功，要么全部失败回滚。场景：UPDATE cs_user SET age = 18 , gender = '女' WHERE id = 4。要么全部更新要么更新失败，不会出现age更新成功，gender更新失败。

**一致性：**据库总是从给一个一致性的状态转换到另一个一致性的状态。场景：比如规定某个表的字段age大于等于12小于18时，字段type为青少年，而数据库中存在age=16的时候，type='儿童'。

**隔离性：**一个事务所做的修改在提交之前对其它事务是不可见的。两个以上的事务不会出现交错执行的状态.因为这样可能会导致数据不一致。

**持久性：**一旦事务提交，其所做的修改便会永久保存在数据库中。



###### 事务的并发问题

　　**1、脏读：**事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据

　　**2、不可重复读：**事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。

　　 **3、幻读：**系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。

　　**小结：**不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表



###### 事务的隔离级别如下：

| **事务隔离级别**             | **脏读** | **不可重复读** | **幻读** |
| ---------------------------- | -------- | -------------- | -------- |
| 读未提交（read-uncommitted） | 是       | 是             | 是       |
| 不可重复读（read-committed） | 否       | 是             | 是       |
| 可重复读（repeatable-read）  | 否       | 否             | 是       |
| 串行化（serializable）       | 否       | 否             | 否       |

##### **1.2 数据库引擎**

  数据库的索引有InnoDB引擎、MyISAM引擎、Archive引擎、CSV引擎、Memory引擎、Federated引擎、Mrg_MyISAM引擎和NDB集群引擎。

**InnoDB引擎：**

1.将数据存储在表空间中，表空间由一系列的数据文件组成，由InnoDB管理；2.支持每个表的数据和索引存放在单独文件中(innodb_file_per_table)；3.支持事务，采用MVCC来控制并发，并实现标准的4个事务隔离级别，支持外键；4.索引基于聚簇索引建立，对于主键查询有较高性能；5.数据文件的平台无关性，支持数据在不同的架构平台移植；6.能够通过一些工具支持真正的热备。如XtraBackup等；7.内部进行自身优化如采取可预测性预读，能够自动在内存中创建hash索引等。

**MyISAM引擎：**

1.MySQL5.1中默认，不支持事务和行级锁；2.提供大量特性如全文索引、空间函数、压缩、延迟更新等；3.数据库故障后，安全恢复性差；4.对于只读数据可以忍受故障恢复，MyISAM依然非常适用；5.日志服务器的场景也比较适用，只需插入和数据读取操作；6.不支持单表一个文件，会将所有的数据和索引内容分别存在两个文件中；7.MyISAM对整张表加锁而不是对行，所以不适用写操作比较多的场景；8.支持索引缓存不支持数据缓存。

**Archive引擎：**

1.只支持insert和select操作；2.缓存所有的写数据并进行压缩存储，支持行级锁但不支持事务；3.适合高速插入和数据压缩，减少IO操作，适用于日志记录和归档服务器。

Blackhole引擎：没有实现任何存储机制，会将插入的数据进行丢弃，但会存储二进制日志。会在一些特殊需要的复制架构的环境中使用。

**CSV引擎：**可以打开CSV文件存储的数据，可以将存储的数据导出，并利用excel打开。可以作为一种数据交换的机制使用。

**Memory引擎：**将数据在内存中缓存，不消耗IO。存储数据速度较快但不会被保留，一般作为临时表的存储被使用。

**Federated引擎：**能够访问远程服务器上的数据的存储引擎。能够建立一个连接连到远程服务器。

**Mrg_MyISAM引擎：**将多个MYISAM表合并为一个。本身并不存储数据，数据存在MyISAM表中间。

**NDB集群引擎**：MySQL Cluster专用。

#### **2、引擎原理**

索引本质：MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。

##### **2.1 MyISAM索引实现**

MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图：

![img](https://img-blog.csdnimg.cn/20190202211031791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyMDAyMjUz,size_16,color_FFFFFF,t_70)

这里设表一共有三列，假设我们以Col1为主键，则图8是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示：
![img](https://img-blog.csdnimg.cn/20190202211259198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyMDAyMjUz,size_16,color_FFFFFF,t_70)

同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。

MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。

##### 2.2 InnoDB索引实现

虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。

第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。![img](https://img-blog.csdnimg.cn/20190202211651621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyMDAyMjUz,size_16,color_FFFFFF,t_70)

是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。

第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，图11为定义在Col3上的一个辅助索引：
![img](https://img-blog.csdnimg.cn/20190202211940789.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyMDAyMjUz,size_16,color_FFFFFF,t_70)

是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。

第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，图11为定义在Col3上的一个辅助索引。

###### **1.聚集索引**

聚集索引是按每张表的主键构造的一颗B+树，并且叶节点中存放着整张表的行记录数据，因此也让聚集索引的节点成为数据页，这个特性决定了索引组织表中数据也是索引的一部分。由于实际的数据页只能按照一颗B+树进行排序，所以每张表只能拥有一个聚集索引。查询优化器非常倾向于采用聚集索引，因为其直接存储行数据，所以主键的排序查询和范围查找速度非常快。
不是物理上的连续，而是逻辑上的，不过在刚开始时数据是顺序插入的所以是物理上的连续，随着数据增删，物理上不再连续。

###### **2.辅助索引**

辅助索引页级别不包含行的全部数据。叶节点除了包含键值以外，每个叶级别中的索引行中还包含了一个书签，该书签用来告诉InnoDB哪里可以找到与索引相对应的行数据。其中存的就是聚集索引的键。
辅助索引的存在并不影响数据在聚集索引的结构组织。InnoDB会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后通过主键索引找到一个完整的行记录。当然如果只是需要辅助索引的值和主键索引的值，那么只需要查找辅助索引就可以查询出索要的数据，就不用再去查主键索引了。

##### 索引使用策略及优化

MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。

###### 最左前缀原理与相关优化

MySQL中的索引可以以一定顺序引用多个列，这种索引叫做联合索引，一般的，一个联合索引是一个有序元组<a1, a2, …, an>，其中各个元素均为数据表的一列。单列索引可以看成联合索引元素数为1的特例。

条件按照 a1,a2...an 的顺序索引才会生效。例如索引3列（a1,a2,a3）

  (1)....where   a1=1 and a2=1 and a3=1    三个列的索引都生效。

  (2)....where   a1=1     a1索引都生效。

  (3)....where   a1=1 and a3=1   a1索引都生效，a3无法生效

(4) 填坑---让a3生效:

    SELECT * FROM employees WHERE a1=1  AND title IN (SELECT DISTINCT(a2) FROM employees)  AND a3=1;

(5)  ....where   a3=1   索引无法生效

(6)....where   a1=1 and a2 like ’po%‘    索引生效 

（7）范围列可以用到索引（必须是最左前缀），但是范围列后面的列无法用到索引。同时，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。

....where   a1<10 and a2 like ’po%‘   a2索引无效 

（8）同时，用了“between”并不意味着就是范围查询。

....where   a1 BETWEEN 1AND  12  AND a2=1 AND a3 BETWEEN 4 AND 8;

看起来是用了两个范围查询，但作用于a1 上的“BETWEEN”实际上相当于“IN”，也就是说a1实际是多值精确匹配。可以看到这个查询用到了索引全部三个列。因此在MySQL中要谨慎地区分多值匹配和范围匹配，否则会对MySQL的行为产生困惑。



### 3**事务的隔离级别实验：**

1、查看数据库事务

mysql> select @@tx_isolation;

2、准备数据

3、读未提交

（1）打开一个客户端A，并设置当前事务模式为read uncommitted（未提交读）查询表age的初始值：

mysql> set session transaction isolation level read uncommitted;
mysql> start transaction;

（2）在客户端A的事务提交之前，打开另一个客户端B，更新表age,但是没有提交。

mysql> set session transaction isolation level read uncommitted;
mysql>  start transaction;

（3）这时，虽然客户端B的事务还没提交，但是客户端A就可以查询到B已经更新的数据：
（4）一旦客户端B的事务因为某种原因回滚，所有的操作都将会被撤销，那客户端A查询到的数据其实就是脏数据：
（5）查看A客户端
所以读未提交的事务隔离级别具有脏读，不可重复度，幻读的特性。

4、读已提交

（1）打开一个客户端A，并设置当前事务模式为read committed（读已提交），查询age


（2）在客户端A的事务提交之前，打开另一个客户端B，更新age：


（3）这时，客户端B的事务还没提交，客户端A不能查询到B已经更新的数据，解决了脏读问题：

（4）客户端B的事务提交

（5）客户端A执行与上一步相同的查询，结果 与上一步不一致，即产生了不可重复读的问题

3、可重复读

（1）打开一个客户端A，并设置当前事务模式为repeatable read，查询表account的所有记录


（2）在客户端A的事务提交之前，打开另一个客户端B，更新

mysql> set session transaction isolation level repeatable read;

(3)A查询，发现没有变化   解决脏读
（4）B提交，后查询A，还是没有变化，，，代表多次读写数据一致。没有出现不可重复读的问题
（5）在客户端A，接着执行update users  age= age - 2 where id = 1，age没有变成12-2=10，而是20 -2 =18，数据的一致性倒是没有被破坏。可重复读的隔离级别下使用了MVCC机制，select操作不会更新版本号，是快照读（历史版本）；insert、update和delete会更新版本号，是当前读（当前版本）。



### 4.串行化

（1）打开一个客户端A，并设置当前事务模式为serializable，查询age的初始值

（2）打开一个客户端B，并设置当前事务模式为serializable，插入一条记录报错，表被锁了插入失败，mysql中事务隔离级别为serializable时会锁表，因此不会出现幻读的情况，这种隔离级别并发性极低，开发中很少会用到。

###### 补充：

　　1、事务隔离级别为读提交时，写数据只会锁住相应的行

　　2、事务隔离级别为可重复读时，如果检索条件有索引（包括主键索引）的时候，默认加锁方式是next-key 锁；如果检索条件没有索引，更新数据时会锁住整张表。一个间隙被事务加了锁，其他事务是不能在这个间隙插入记录的，这样可以防止幻读。

　　3、事务隔离级别为串行化时，读写数据都会锁住整张表

　　 4、隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。

 

**READ UNCOMMITTED (未提交读) ：**隔离级别：0.  哪个问题都不能解决

原理： 事务A和事务B，事务B可以读取事务A未提交的记录。会出现脏读，因为事务A可能会回滚操作，导致数据发生变化。

**READ COMMITTED (提交读) ：**隔离级别：1.   可以解决脏读 。

  原理： 事务中只能看到已提交的修改，提交读这种隔离级别保证了读到的任何数据都是提交的数据，避免了脏读，但是不保证事务重新读的时候能读到相同的数据，因为在每次数据读完之后其他事务可以修改刚才读到的数据。

**REPEATABLE READ (可重复读) ：**隔离级别：2. 可以解决脏读和不可重复读，实现不幻读，需要加锁

原理：在InnoDB中是这样的：RR隔离级别保证对读取到的记录加锁 (记录锁)，同时保证对读取的范围加锁，新的满足查询条件的记录不能够插入 (间隙锁)，因此不存在幻读现象。但是标准的RR只能保证在同一事务中多次读取同样记录的结果是一致的，而无法解决幻读问题。InnoDB的幻读解决是依靠MVCC的实现机制做到的。Mysql默认的隔离级别是RR。

**InnoDB的幻读解决是依靠MVCC的实现机制：**   （增加系统版本号，每次事务操作，会比较系统版本号）                      
        InnoDB为每行记录添加了一个版本号（系统版本号），每当修改数据时，版本号加一。在读取事务开始时，系统会给事务一个当前版本号，事务会读取版本号<=当前版本号的数据，这时就算另一个事务插入一个数据，并立马提交，新插入这条数据的版本号会比读取事务的版本号高，因此读取事务读的数据还是不会变。例如：此时books表中有5条数据，版本号为1  事务A，系统版本号2：select * from books；因为1<=2所以此时会读取5条数据。 事务B，系统版本号3：insert into books ...，插入一条数据，新插入的数据版本号为3，而其他的数据的版本号仍然是2，插入完成之后commit，事务结束。  事务A，系统版本号2：再次select * from books；只能读取<=2的数据，事务B新插入的那条数据版本号为3，因此读不出来，解决了幻读的问题。

**SERIALIZABLE （可串行化）：**隔离级别：3.

原理：该隔离级别会在读取的每一行数据上都加上锁，退化为基于锁的并发控制，即LBCC。可以解决脏读不可重复读和幻读—相当于锁表 

**需要注意的是，MVCC只在RC和RR两个隔离级别下工作，其他两个隔离级别都和MVCC不兼容。**

### Mysql索引命中规则

#### 最左匹配原则 

​                 1、先定位该sql的查询条件，有哪些，那些是等值的，那些是范围的条件。 
​                  2、等值的条件去命中索引最左边的一个字段，然后依次从左往右命中，范围的放在最后。



#### 一条sql语句要执行完成需要经历什么样的过程

    当一条sql语句提交给mysql数据库进行查询的时候需要经历以下几步 
      1、先在where解析这一步把当前的查询语句中的查询条件分解成每一个独立的条件单元 
      2、mysql会自动将sql拆分重组 
      3、然后where条件会在B-tree index这部分进行索引匹配，如果命中索引，就会定位到指定的table records位置。如果没有命中，则只能采用全部扫描的方式 
      4、根据当前查询字段返回对应的数据值
![img](https://img-blog.csdnimg.cn/20190202225335210.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyMDAyMjUz,size_16,color_FFFFFF,t_70)

#### Undo原理：（备份旧数据）

         在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为Undo Log）。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。

#### Redo原理：（保存最新数据）

         和Undo Log相反，Redo Log记录的是新数据的备份。在事务提交前，只要将Redo Log持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是Redo Log已经持久化。系统可以根据Redo Log的内容，将所有数据恢复到最新的状态。

#### MVCC的设计目的是什么，怎么使用版本号判断数据的可见性

​        MVCC是一种多版本并发控制机制。锁机制可以控制并发操作,但是其系统开销较大,而MVCC可以在大多数情况下代替行级锁,使用MVCC,能降低其系统开销。

   人们一般把基于锁的并发控制机制称成为悲观机制，而把MVCC机制称为乐观机制。这是因为锁机制是一种预防性的，读会阻塞写，写也会阻塞读，当锁定粒度较大，时间较长时并发性能就不会太好；而MVCC是一种后验性的，读不阻塞写，写也不阻塞读，等到提交的时候才检验是否有冲突，由于没有锁，所以读写不会相互阻塞，从而大大提升了并发性能。

   MVCC的一种简单实现是基于CAS（Compare-and-swap）思想的有条件更新（Conditional Update）。普通的update参数只包含了一个keyValueSet’，Conditional Update在此基础上加上了一组更新条件conditionSet { … data[keyx]=valuex, … }，即只有在D满足更新条件的情况下才将数据更新为keyValueSet’；否则，返回错误信息。

————————————————
版权声明：本文为CSDN博主「海鸥-号」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_32002253/article/details/86748194



# Mysql工作原理介绍

### Mysql 工作原理图

Mysql是由SQL接口，解析器，优化器，缓存，存储引擎组成的。

![img](https://img-blog.csdnimg.cn/20191104172356617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzk5MTAyNw==,size_16,color_FFFFFF,t_70)

### mysql原理图各个组件说明：

connectors
与其他编程语言中的sql 语句进行交互，如php、java等。
Management Serveices & Utilities
系统管理和控制工具
Connection Pool (连接池)
管理缓冲用户连接，线程处理等需要缓存的需求
SQL Interface (SQL接口)接受用户的SQL命令，并且返回用户需要查询的结果。比如select from就是调用SQL Interface
Parser （解析器）SQL命令传递到解析器的时候会被解析器验证和解析。
主要功能：
a . 将SQL语句分解成数据结构，并将这个结构传递到后续步骤，后面SQL语句的传递和处理就是基于这个结构的
b. 如果在分解构成中遇到错误，那么就说明这个sql语句是不合理的，语句将不会继续执行下去
Optimizer (查询优化器)
SQL语句在查询之前会使用查询优化器对查询进行优化(产生多种执行计划,最终数据库会选择最优化的方案去执行,尽快返会结果) 他使用的是“选取-投影-联接”策略进行查询。
用一个例子就可以理解： select uid,name from user where gender = 1;
这个select 查询先根据where 语句进行选取，而不是先将表全部查询出来以后再进行gender过滤
这个select查询先根据uid和name进行属性投影，而不是将属性全部取出以后再进行过滤将这两个查询条件联接起来生成最终查询结果.
Cache和Buffer (查询缓存)
如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据。
这个缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，key缓存，权限缓存等
Engine (存储引擎)
存储引擎是MySql中具体的与文件打交道的子系统。也是Mysql最具有特色的一个地方。
Mysql的存储引擎是插件式的。它根据MySql AB公司提供的文件访问层的一个抽象接口来定制一种文件访问机制（这种访问机制就叫存储引擎）

### SQL 语句执行过程

数据库通常不会被直接使用，而是由其他编程语言通过SQL语句调用mysql，由mysql处理并返回执行结果。那么Mysql接受到SQL语句后，又是如何处理
首先程序的请求会通过mysql的connectors与其进行交互，请求到处后，会暂时存放在连接池（connection pool)中并由处理器（Management Serveices & Utilities）管理。当该请求从等待队列进入到处理队列，管理器会将该请求丢给SQL接口（SQL Interface）。SQL接口接收到请求后，它会将请求进行hash处理并与缓存中的结果进行对比，如果完全匹配则通过缓存直接返回处理结果；否则，需要完整的走一趟流程：
(1)由SQL接口丢给后面的解释器（Parser），解释器会判断SQL语句正确与否，若正确则将其转化为数据结构。
(2)解释器处理完，便来到后面的优化器（Optimizer），它会产生多种执行计划,最终数据库会选择最优化的方案去执行,尽快返会结果。
(3)确定最优执行计划后，SQL语句此时便可以交由存储引擎（Engine）处理，存储引擎将会到后端的存储设备中取得相应的数据，并原路返回给程序。

#### 注意点

(1)如何缓存查询数据
存储引擎处理完数据，并将其返回给程序的同时，它还会将一份数据保留在缓存中，以便更快速的处理下一次相同的请求。具体情况是，mysql会将查询的语句、执行结果等进行hash，并保留在cache中，等待下次查询。
(2)buffer与cache的区别
从mysql原理图可以看到，缓存那里实际上有buffer和cache两个，那它们之间的区别：简单的说就是，buffer是写缓存，cache是读缓存。
(3)如何判断缓存中是否已缓存需要的数据
这里可能有一个误区，觉得处理SQL语句的时候，为了判断是否已缓存查询结果，会将整个流程走一遍，取得执行结果后再与需要的进行对比，看看是否命中，并以此说，既然不管缓存中有没有缓存到查询内容，都要整个流程走一遍，那缓存的优势在哪？
其实并不是这样，在第一次查询后，mysql便将查询语句以及查询结果进行hash处理并保留在缓存中，SQL查询到达之后，对其进行同样的hash处理后，将两个hash值进行对照，如果一样，则命中，从缓存中返回查询结果；否则，需要整个流程走一遍。

### MySQL逻辑架构整体分为三层：

**1.最上层是一些客户端和连接服务**，包含本地sock通信和大多数基于客户端/服务端工具实现的类似于tcp/ip的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。
**2.第二层架构主要完成大多少的核心服务功能**，如SQL接口，并完成缓存的查询，SQL的分析和优化及部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定查询表的顺序，是否利用索引等，最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存。如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。
**3.存储引擎层**，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。

### 并发控制和锁的概念

当数据库中有多个操作需要修改同一数据时，不可避免的会产生数据的脏读。这时就需要数据库具有良好的并发控制能力，这一切在MySQL中都是由服务器和存储引擎来实现的。
解决并发问题最有效的方案是引入了锁的机制，锁在功能上分为共享锁(shared lock)和排它锁(exclusive lock)即通常说的读锁和写锁。当一个select语句在执行时可以施加读锁，这样就可以允许其它的select操作进行，因为在这个过程中数据信息是不会被改变的这样就能够提高数据库的运行效率。当需要对数据更新时，就需要施加写锁了，不在允许其它的操作进行，以免产生数据的脏读和幻读。锁同样有粒度大小，有表级锁(table lock)和行级锁(row lock)，分别在数据操作的过程中完成行的锁定和表的锁定。这些根据不同的存储引擎所具有的特性也是不一样的。
MySQL大多数事务型的存储引擎都不是简单的行级锁，基于性能的考虑，他们一般都同时实现了多版本并发控制(MVCC)。这一方案也被Oracle等主流的关系数据库采用。它是通过保存数据中某个时间点的快照来实现的，这样就保证了每个事务看到的数据都是一致的。详细的实现原理可以参考《高性能MySQL》第三版。



## **MySql优化**

 服务器配置的优化

1. Mysql配置的优化，连接数的设置等。
2. 架构的优化：使用redis缓存，集群，主从复制，分库分表
3. Sql的优化：慢查询日志分析，分析mysql运行状态，explain分析sql，看看查询顺序、查询type类型(system>const>eq_ref>ref>range>index>all 至少达到range范围索引)，可能用到的索引，扫描行数，extra信息，优化sql语句，比如使用索引，
4. 存储引擎的选择，常规的innodb，查询多的myisam，临时数据多的memory，表结构的构建，字段的类型选择，比如性别用tinyint，varchar代替char，不要使用外键，触发器，视图等，不要使用数据库存图片。
5. 代码的优化，降级限流，消息队列等等，尽量减轻数据库压力。

#### 1.一条sql语句如何执行的：mysql5.7查询缓存默认关闭，mysql8缓存已被移除。

![img](https://img2020.cnblogs.com/blog/1275221/202010/1275221-20201024081705891-1816810466.png)

MySIAM：表级锁定，不支持事务，已读为主

InnoDB：支持事务，支持外键，支持行级别和表级别的锁定，B+索引，效率高

Memory：内存存储。

Archive：用于存储和检索大量很少引用的历史、存档、安全审计信息，不支持事务。

1. mysql架构
2. ![img](https://img2020.cnblogs.com/blog/1275221/202010/1275221-20201024081717345-1557954580.png)

局部性原理：读取磁盘的数据，它附近的数据也会被读取到内存，操作形同一般4k，mysql中读取每页的数据16k，Innodb设置了一个内存叫Buffer Pool，默认128M，读取数据时先去buffer中读取，写数据也是先写到buffer中，为了不产生脏页，innodb有专门的后台线程把buffer中的数据写到磁盘中，这个动作叫刷脏。为了防止数据库宕机造成数据不一致，Innodb会把修改操作写到Redo log（重做日志）中，事务的acid的持久化就是这个日志实现的，(为啥不直接写到磁盘，因为写磁盘的数据不是连续的，io效率低，日志是顺序的，快速。)redo log大小是固定的，一旦写满，就会触发buffer到磁盘的同步。和redo log 还有一个undo log 是重做日志，记录的修改之前的数据，用于回滚。

 

6.redo log 和undo log是innodb独有的，mysql的server层也有一个日志叫Binlog可以被所有存储引擎使用，binlog以事件的形式记录所有ddl和dml语句，比如给id=1的count字段加1，binlog记录的是操作不是数据值，属于逻辑日志。

Binlog用于1.主从复制2数据恢复。

7.mysql主从复制原理:从服务器读取主服务器的binlog，在执行一次。![image-20210819104739911](C:\Users\86132\AppData\Roaming\Typora\typora-user-images\image-20210819104739911.png)

7．一条更新语句的执行（省略了undo log）![img](https://img2020.cnblogs.com/blog/1275221/202010/1275221-20201024081755631-1887839302.png)

![img](https://img2020.cnblogs.com/blog/1275221/202010/1275221-20201024081806186-30570661.png)





# [MySQL — 优化之explain执行计划详解（转）](https://www.cnblogs.com/myseries/p/10736268.html)

**EXPLAIN简介**

　　EXPLAIN 命令是查看查询优化器如何决定执行查询的主要方法,使用EXPLAIN,只需要在查询中的SELECT关键字之前增加EXPLAIN这个词即可,MYSQL会在查询上设置一个标记,当执行查询时,这个标记会使其返回关于在执行计划中每一步的信息,而不是执行它,它会返回一行或多行信息,显示出执行计划中的每一部分和执行的次序,从而可以从分析结果中找到查询语句或是表结构的性能瓶颈。

**EXPLAIN能干嘛**

1. 分析出表的读取顺序
2. 数据读取操作的操作类型
3. 哪些索引可以使用
4. 哪些索引被实际使用
5. 表之间的引用
6. 每张表有多少行被优化器查询  

**EXPLAIN如何用**

Explain + SQL语句即可,如下:

```
explain select * from tbl_dept;
```

- 1执行结果如下:

*[![这里写图片描述](https://img-blog.csdn.net/20180521155012377?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poMTU3MzI2MjE2Nzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)](https://img-blog.csdn.net/20180521155012377?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poMTU3MzI2MjE2Nzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)*

**EXPLAIN结果参数含义**

1.id: id代表执行select子句或操作表的顺序,例如,上述的执行结果代表只有一次执行而且执行顺序是第一(因为只有一个id为1的执行结果),id分别有三种不同的执行结果,分别如下:

- id相同,执行顺序由上至下

[![img](https://img2018.cnblogs.com/blog/885859/201904/885859-20190419152720756-229903142.png)](https://img2018.cnblogs.com/blog/885859/201904/885859-20190419152720756-229903142.png)

- id不同,如果是子查询,id的序号会递增,id值越大,优先级越高,越先被执行

[![img](https://img2018.cnblogs.com/blog/885859/201904/885859-20190419152812436-124187623.png)](https://img2018.cnblogs.com/blog/885859/201904/885859-20190419152812436-124187623.png)

- id相同和不同,同时存在,遵从优先级高的优先执行,优先级相同的按照由上至下的顺序执行

[![img](https://img2018.cnblogs.com/blog/885859/201904/885859-20190419152835637-1233280197.png)](https://img2018.cnblogs.com/blog/885859/201904/885859-20190419152835637-1233280197.png)

**2.select_type**
　　查询的类型,主要用于区别普通查询,联合查询,子查询等复杂查询

- simple: 简单的select查询,查询中不包含子查询或union查询
- primary: 查询中若包含任何复杂的子部分,最外层查询则被标记为primary
- subquery 在select 或where 列表中包含了子查询
- derived 在from列表中包含的子查询被标记为derived,mysql会递归这些子查询,把结果放在临时表里
- union 做第二个select出现在union之后,则被标记为union,若union包含在from子句的子查询中,外层select将被标记为derived
- union result 从union表获取结果的select

**3.table**
　　显示一行的数据时关于哪张表的
**4.type**
　　查询类型从最好到最差依次是:system>const>eq_ref>ref>range>index>All,一般情况下,得至少保证达到range级别,最好能达到ref

- **system**:表只有一行记录,这是const类型的特例,平时不会出现
- **const**:表示通过索引一次就找到了,const即常量,它用于比较primary key或unique索引,因为只匹配一行数据,所以效率很快,如将主键置于where条件中,mysql就能将该查询转换为一个常量 

[![img](https://img2018.cnblogs.com/blog/885859/201904/885859-20190419153049676-1370075656.png)](https://img2018.cnblogs.com/blog/885859/201904/885859-20190419153049676-1370075656.png)

- **eq_ref**:唯一性索引扫描,对于每个索引键,表中只有一条记录与之匹配,常见于主键或唯一索引扫描
- **ref**:非唯一性索引扫描,返回匹配某个单独值的行,它可能会找到多个符合条件的行,所以他应该属于查找和扫描的混合体
- **range**:只检索给定范围的行,使用一个索引来选择行,如where语句中出现了between,<,>,in等查询,这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。
- **index**:index类型只遍历索引树,这通常比All快,因为索引文件通常比数据文件小,index是从索引中读取,all从硬盘中读取
- **all**:全表扫描,是最差的一种查询类型

**5.possible_keys**
　　显示可能应用在这张表中的索引,一个或多个,查询到的索引不一定是真正被用到的

**6.key**
　　实际使用的索引,如果为null,则没有使用索引,因此会出现possible_keys列有可能被用到的索引,但是key列为null,表示实际没用索引。

**7.key_len**
　　表示索引中使用的字节数,而通过该列计算查询中使用的 索引长度,在不损失精确性的情况下,长度越短越好,key_len显示的值为索引字段的最大可能长度,并非实际使用长度,即,key_len是根据表定义计算而得么不是通过表内检索出的

**8.ref**
　　显示索引的哪一列被使用了,如果可能的话是一个常数,哪些列或常量被用于查找索引列上的值

**9.rows**
　　根据表统计信息及索引选用情况,大只估算出找到所需的记录所需要读取的行数

**10.Extra**

- **Using filesort**:说明mysql会对数据使用一个外部的索引排序,而不是按照表内的索引顺序进行读取,mysql中无法利用索引完成的排序操作称为"文件排序"
- **Using temporary** :使用了临时表保存中间结果,mysql在对查询结果排序时使用临时表,常见于order by和分组查询group by
- **Using index**:表示相应的select操作中使用了覆盖索引（Covering Index），避免访问了表的数据行，效率不错。如果同时出现using where，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表明索引用来读取数据而非执行查找动作。 其中的覆盖索引含义是所查询的列是和建立的索引字段和个数是一一对应的
- **Using where**:表明使用了where过滤
- **Using join buffer**:表明使用了连接缓存,如在查询的时候会有多次join,则可能会产生临时表
- **impossible where**:表示where子句的值总是false,不能用来获取任何元祖。如下例：

```
select * from t1 where id='1' and id='2';
```

- select tables optimized away

在没有GROUPBY子句的情况下，基于索引优化MIN/MAX操作或者对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。

- distinct：优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作，即一旦MySQL找到了与行相联合匹配的行，就不再搜索了。

**重点：**

　　**type**：访问类型，查看SQL到底是以何种类型访问数据的。

　　**key**：使用的索引，MySQL用了哪个索引，有时候MySQL用的索引不是最好的，需要force index()。

　　**rows**：最大扫描的列数。

　　**extra**：重要的额外信息，特别注意损耗性能的两个情况，using filesort和using temporary。



# [mysql实现开窗函数、Mysql实现分析函数](https://www.cnblogs.com/gered/p/10430829.html)

https://www.cnblogs.com/gered/p/10430829.html

```sql
-- 测试数据
CREATE TABLE `tem` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`str` char(1) DEFAULT NULL,
PRIMARY KEY (`id`)
) ;


INSERT INTO `test`.`tem`(`id`, `str`) VALUES (1, 'A');
INSERT INTO `test`.`tem`(`id`, `str`) VALUES (2, 'B');
INSERT INTO `test`.`tem`(`id`, `str`) VALUES (3, 'A');
INSERT INTO `test`.`tem`(`id`, `str`) VALUES (4, 'C');
INSERT INTO `test`.`tem`(`id`, `str`) VALUES (5, 'A');
INSERT INTO `test`.`tem`(`id`, `str`) VALUES (6, 'C');
INSERT INTO `test`.`tem`(`id`, `str`) VALUES (7, 'B');


【1】row_number() over(order by )
变量会最后再计算，所以是先排序好之后，才会开始计算@num

复制代码
SELECT
    @num := @num+1 num,
    id,
    str
FROM
    tem, (SELECT @str := '', @num := 0) t1
ORDER BY
    str, id;
    
    
    【2】实现分组排名效果（row_number() over(partition by order by )）
复制代码
--变量方式
SELECT
    @num := IF(@str = str, @num + 1, 1) num,
    id,
    @str := str str
FROM
    tem, (SELECT @str := '', @num := 0) t1
ORDER BY
    str, id;
```





# mysql 试题 

https://blog.csdn.net/java_bibug/article/details/108630368?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control



# [时间复杂度o(1), o(n), o(logn), o(nlogn)](https://www.cnblogs.com/biehongli/p/11672380.html)

1、时间复杂度o(1), o(n), o(logn), o(nlogn)。算法时间复杂度有的时候说o(1), o(n), o(logn), o(nlogn)，这是算法的时空复杂度的表示。不仅仅用于表示时间复杂度，也用于表示空间复杂度。O后面的括号中有一个函数，指明某个算法的耗时/耗空间与数据增长量之间的关系。其中的n代表输入数据的量。

　　大O描述的是算法的运行时间和输入数据之间的关系。



 

2、时间复杂度为O(1)。
　　是最低的时空复杂度了，也就是耗时/耗空间与输入数据大小无关，无论输入数据增大多少倍，耗时/耗空间都不变。
哈希算法就是典型的O(1)时间复杂度，无论数据规模多大，都可以在一次计算后找到目标（不考虑冲突的话）。

 

3、时间复杂度为O(n)。
　　就代表数据量增大几倍，耗时也增大几倍。
比如常见的遍历算法。再比如时间复杂度O(n^2)，就代表数据量增大n倍时，耗时增大n的平方倍，这是比线性更高的时间复杂度。
比如冒泡排序，就是典型的O(n^2)的算法，对n个数排序，需要扫描n×n次。

 

4、时间复杂度为O(logn)。
　　当数据增大n倍时，耗时增大logn倍（这里的log是以2为底的，比如，当数据增大256倍时，耗时只增大8倍，是比线性还要低的时间复杂度）。
二分查找就是O(logn)的算法，每找一次排除一半的可能，256个数据中查找只要找8次就可以找到目标。

指数函数：一般地，y=a^x函数(a为常数且以a>0，a≠1)叫做指数函数。y=a^x表示a的x次方。
对数函数：如果a^x =N（a>0，且a≠1），那么数x叫做以a为底N的对数，记作x=logaN，读作以a为底N的对数，其中a叫做对数的底数，N叫做真数。


5、时间复杂度为O(nlogn)。
　　就是n乘以logn，当数据增大256倍时，耗时增大256*8=2048倍。这个复杂度高于线性低于平方。
归并排序就是O(nlogn)的时间复杂度。

# 从I/O到索引的那些事

## 前言

大多数项目的查询操作占据了数据处理很大的比例，关于查询的优化成为了很多数据库一直研究的重点。当前的数据库产品一旦涉及到超大库数据的查询都会采用索引技术，如MySql、Oracle、SqlServer、Hive...在满足不同的产品特性和应用场景里有着不同的实现方案。

通常来说，索引的目的主要是提高查询速度，不同数据库对索引的技术细节做了很好的封装，在实际应用中对于开发或维护人员来说是完全透明的。然而，笔者认为，深入理解查询技术对于开发者来说是面向大型数据处理的必要过程；对于架构人员来说，索引技术的充分理解往往可以在数据处理方案的选择上进行全面的分析，做出良好判断。

本文从操作系统层面结合硬件I/O过程理解查询所带来的关键问题，从而引申出索引查询技术的设计思路，整个过程将对索引关于I/O层面的技术细节做出分析。

## I/O的认识

计算机经过几十年发展各方面性能都得到了显著提高，然而却一直有个因素限制着计算机的高速运转，这就是I/O的问题。

很多开发者似乎并不对I/O有刻意的认识，但在数据处理领域这是一个绕不开的话题，关于I/O维基百科是这样解释的：

> **I/O**（英语：**I**nput/**O**utput），即**输入/输出**，通常指数据在[内部存储器](https://link.juejin.cn/?target=https%3A%2F%2Fzh.wikipedia.org%2Fwiki%2F%E5%86%85%E9%83%A8%E5%AD%98%E5%82%A8%E5%99%A8)和外部存储器或其他周边设备之间的输入和输出。I/O是信息处理系统（例如计算机）与外部世界（可能是人类或另一信息处理系统）之间的通信。输入是系统接收的信号或数据，输出则是从其发送的信号或数据。

在了解I/O前，我们需要认识的是，现代计算机，磁盘和内存一直占据着主要角色，大部分持久化的数据都会选择容量较大、成本较低的磁盘中，而内存则作为高速缓存的角色为计算机提供存储支持。在数据查询中，一次I/O可以理解为从磁盘亦或是从内存读取一次数据的过程，在涉及到查询性能的分析中，就必须对I/O的成本具备一定的敏感性。

**什么是I/O的成本？**

计算机性能的大幅提升离不开存储器和CPU在处理速度上的不断提高，但关于性能提升的速度在CPU和存储器间却造成了一定的差距，CPU自1980年来在普通计算机中的处理速度已经实现了10000倍的提升，而磁盘只提升了30倍左右。这造成了一种现象，就是大多数时候CPU需要等待磁盘的运转，一个完整的数据处理过程，磁盘往往占据了主要的操作时间，在这里我们可以把I/O的成本理解为每一次从磁盘、内存读取的时间消耗。

之前一篇文章【[理解进程的存在](https://juejin.cn/post/6844903639111188487)】笔者分析了CPU处理运算的基本过程，从CPU每秒亿万次的时钟周期中我们具备了一定的感知，就是CPU对指令的处理速度大大超越了常规理解，底有多快？当前4核CPU的时钟周期已经达到了0.4ns左右，也就是说每0.4ns就能完成一次指令操作，而内存一次访问时间大概是9ns（建议理解下ns、ms这些维度的时间差距），尽管差了20来倍，但似乎还维持在同一量级的水平上。磁盘就离谱了，机械磁盘当前还维持在29ms的访问时间，和内存、CPU分别是百万和千万量级的巨大差异！磁盘相较于内存为何在I/O中有如此糟糕的表现？参考之前文章【[磁盘和内存的基本认识](https://juejin.cn/post/6844903505224826887)】的相关描述，我们这可以从存储器的存储原理和存储介质两方面做好了解。

因此，数据处理的瓶颈往往在于I/O，而I/O的瓶颈往往在于驱动硬盘的过程，只有尽可能减少对磁盘的I/O依赖，我们才能从性能上完成突破。

需要清楚的是，磁盘较差的I/O表现不代表不能被市场接收。因为磁盘的制造成本相较于其它高速存储器有着巨大优势，所以接下来很长一段时间磁盘的I/O依旧是一个需要面对的课题。

**从OS层面看磁盘I/O过程**

在硬件角度上我们认识到存储器本身带来的局限性，那么大部分场景下关于查询的优化将主要从软件层面做好处理，不同操作系统在底层上都对I/O做了良好支持，量化I/O成本必须从OS层面进行细致了解。

早期计算机系统的存储层次只有三层：CPU寄存器、DRAM主存储器（内存）和磁盘存储，OS围绕这三种存储介质进行相应优化，由于CPU和存储器在发展过程中性能差距不断变大，目前计算机在寄存器和内存之间又置入了多层高速缓存存储器，部分情况下内存和磁盘中间还会置入SSD（固态硬盘）。当前OS利用一定的算法在缓存命中方面做好工作，尽可能提高整体I/O的性能，但这并不妨碍我们核心问题的分析，大多数情况下，OS需要解决的问题依旧是数据从磁盘到内存的过程，为了简化分析模型，我们将问题从内存、磁盘两者进行思考。

在磁盘中，数据是以块为单位进行管理的，每块一般设定为4Kb，我们可以抽象出磁盘就是很多块按序号排列的存储结构：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/8/2/164f8955f2b9eef7~tplv-t2oaga2asx-watermark.awebp)

当OS需要获取磁盘某个数据时，将产生一个I/O中断，本次I/O中断将带上具体的块号去驱动磁盘进行块数据查找和读取操作，这些操作都是以某块作为起点，每次读取数据的最小单位也是4Kb，这意味着如果你获取的数据小于4Kb或者数据放置在某块特定偏移处，那么磁盘依然会从该块开始将整个块内容传递到内存中：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/8/2/164f8abce48e7d2d~tplv-t2oaga2asx-watermark.awebp)

对于小于4Kb的数据，如果刚好放置在某块中，那么往往只需对磁盘进行一次驱动，也就是一次I/O过程，但如果这数据刚好横跨两个块，那么就需要驱动磁盘读取两个块数据了（下图查询目标内容需要读取块N、块N+1）：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/8/2/164f8b8500715755~tplv-t2oaga2asx-watermark.awebp)

因此，我们应该尽量避免上述情况，将目标数据以块为单位进行对齐，这样能减少磁盘I/O过程中块的读取次数。

但如果目标数据本身就很大，必须占用多个块呢？显然，假设目标数据连续占用N个块，那么磁盘将进行N次读取操作：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/8/2/164f8c346adecfe9~tplv-t2oaga2asx-watermark.awebp)

但如果目标数据并不顺序存放，而是分散在各个块区域：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/8/2/164f941f574bd839~tplv-t2oaga2asx-watermark.awebp)

同样是进行三个块读取操作，但从磁盘的物理构造来讲，磁盘必须进程两次块定位操作，也就是I/O中的磁头定位过程。磁盘在读取目标数据前必须将磁头定位到指定的块起始处，如果中间目标数据分散在块的不同区域，那么磁头必须进行一定距离的物理旋转工作，这显然会占用读取时间，也就是顺序读和随机读的关键区别！因此，最理想的情况是，目标数据在磁盘中的存放位置不仅块对齐而且是连续存放，至少要控制数据在磁盘的离散程度在较低水平。

CPU在执行任何指令时，但凡涉及到磁盘I/O，OS都会将数据先预读到内存缓存空间，通过上文的介绍，内存的存取周期和CPU的处理周期在量级上并无太大区别，因此数据库在管理数据方法上，更多的精力花在减少磁盘的I/O。

## 数据的管理和检索

数据库对不同表会有专门的文件管理，一个文件可以理解为记录的一个序列，不同表在磁盘中的文件组织方式可能有一定区别。常规来讲，诸如mysql的关系型数据库是一种行式数据库，表中每条记录可以理解为一行，每行不同字段的内容在磁盘中是顺序存放的，而不同行在磁盘的的位置则不一定是按序的，有可能分散在不同区域的块中。

我们考虑在数据库中创建一个用户表user：

```
CREATE TABLE `user` (
  `id` varchar(20),
  `name` varchar(20),
  `age` numeric(3,0)
)复制代码
```

假设数据库给每个字段分配了最大容量，即id（20个字节）、name（20字节）、age(2字节），我们创建了如下3条记录：

| ***记录1\*** | 1    | cary  | 25   |
| ------------ | ---- | ----- | ---- |
| ***记录2\*** | 2    | harry | 26   |
| ***记录3\*** | 3    | marry | 23   |



我们可知每行记录都占用了固定大小42Byte，一开始3条记录还是顺序存放的：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/8/2/164f9801816a6742~tplv-t2oaga2asx-watermark.awebp)

当我们执行查询操作：

```
select * from user where id=2;复制代码
```

显然， 本次查询将对应块内容加载到内存后开始按行处理，筛选出所有id=2的数据，CPU执行模型如下 ：

```
do begin
    for each row in user {
         if row.id=2 {
             select row;
         }
    }
end复制代码
```

整个过程的I/O复杂度为O(1)、CPU计算复杂度为O(3)。现在我们假设每行数据依然按序存放，但行数扩增为100万行：

| 记录1                   | 1       | cary  | 25   |
| ----------------------- | ------- | ----- | ---- |
| 记录2                   | 2       | harry | 26   |
| 记录3                   | 3       | marry | 23   |
| ....... ....... ....... |         |       |      |
|                         |         |       |      |
| 记录999999              | 999999  | joke  | 28   |
| 记录1000000             | 1000000 | zerui | 26   |

在不考虑块对齐的情况下，100万行数据将占用max(1000000*42Byte/4Kb)=42000个块，同样执行上述sql查询的情况下，I/O的复杂度为O(42000)，CPU计算复杂度为O(1000000)。在如此量级条件下机器处理压力将大幅上升，并且随着表数据的不断增加，复杂度呈线性增加状态，这绝对是不可接受的！

因此，必须要有一种技术来解决大量数据的检索问题，我们关注两个核心需求，一个是减少磁盘的I/O、一个是降低CPU的处理复杂度，索引应运而生。

## 索引的介绍

我们现在已经对数据查询有了基本的成本概念，这个成本体现在I/O和CPU处理上，索引如何解决这两个问题呢？

上文示例的user表数据结构中，我们假设了行数据在磁盘中按id顺序存放，当我们按id条件进行数据检索时，最简单的方式其实可以新增一个数据结构来做目标区间的定位：

| id值    | 目标记录块号 |
| ------- | ------------ |
| 1       | N            |
| 10000   | N+K          |
| 20000   | N+2K         |
| ……      |              |
| 1000000 | N+100K       |

 (N 、K 为整数）

该结构描述了一个映射条件，左边是表id属性值，右边是对应记录在磁盘中的块号，当我们要查询id=100000的记录时，通过映射表我们可以预判目标记录放置在磁盘块区间[N+10K，N+11K]，因此接下来OS只需驱动磁盘进行最多1K个块的读取操作，整个过程I/O复杂度最大为O(1K)，CPU处理复杂度最大为O(10000)，处理性能实现了10000倍的提高！

这样简单设计的数据结构我们称为顺序索引，索引信息同样放置在磁盘空间管理，但凡对id字段的检索，数据库会优先考虑从该索引表进行目标块定位，然后从目标块中检索目标信息。按照上诉设计的索引结构，区间划分的粒度为10000，可知总共有100行，假设每行记录我们设定10Byte空间存储，那么整个索引结构将占用1000Byte，不到1Kb，因此有些时候我们可以直接将索引结构预加载到内存，这样关于索引的查询过程将不涉及到磁盘的I/O消耗，这为我们优化查询速度提供了新的思路！

但是，顺序索引结构的简单是基于行记录在磁盘存储上的苛刻要求下所支持的，行记录必须按照id值在磁盘中按序存放。现实业务环境下，数据有较高的复杂度和变动频率，一些行数据会被删除，腾出的空间会被其它后续插入的行给占用，后续插入的记录在磁盘中也可能呈现随机状态。而一旦目标行数据不按顺序存放，那么顺序索引关于块区间的划分将毫无意义！

| 记录1 | 1    | cary  | 25   |                                                   |
| ----- | ---- | ----- | ---- | ------------------------------------------------- |
| 记录2 | 2    | harry | 26   |                                                   |
| 记录3 | 897  | karry | 24   | **原来id=3的记录被删除，后续插入了id为897的记录** |

这里，我们介绍新的一种数据结构：二叉树，当目标数据在磁盘的存放呈现离散随机状态时，我们依然希望能很好地实现快速检索。

在二叉树的基础上我们增加如下条件：左子节点小于根节点，右子节点大于或等于根节点，假设user表有id为2、3、5、6、7、8的记录，那么将有如下形态：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/8/4/16503ef48a10fcb5~tplv-t2oaga2asx-watermark.awebp)

由此构建的索引结构包含了id属性所有值的情况，对100万行的数据来说，该索引结构就有100万个节点，每个节点还包含了该id对应记录在磁盘中的具体位置，当查询id=3的记录时，我们发现经过根节点的判断后，左指针刚好指向节点为3的内容，由此可获取记录最终的磁盘位置。

那么如何分析二叉树索引结构带来的成本问题？通过该结构规则，其实相当于对记录进行了二分法操作，从数学的角度讲，每次判断都是一次半数的数量级检索。上述示例总共6条记录我们最多只需进行**log**2**N**=6即N=3的判断次数，对于100万个节点的索引结构，我们查询id最多需要**log**2**N**=1000000即N=20的节点获取，也就是磁盘I/O复杂度最大为O(20)，假设目标记录增长为1000万行，I/O复杂度也不过是O(23)，I/O复杂度和目标记录行数呈现出的指数关系极大缓解了I/O成本上升的趋势！

I/O复杂度和二叉树的高度是存在直接关系的，100万个节点需要构造最多20层高度的二叉树，假设每个节点内容包含：左指针（4Byte）、id值（8Byte）、右指针（4Byte）、目标记录指针（4Byte），总共占据20Byte，那么索引将至少占据20Byte*1000000=20Mb的存储空间，最理想的情况是将索引结构直接加载到内存，这样只需在内存消耗O(20)，但区别于不同的机器性能，20Mb对于珍贵的内存资源还是稍显奢侈，能否在磁盘上减少索引层面的I/O次数呢？

我们在二叉树索引构造中，每次I/O的成本带来的是一半数据的过滤功效，在此基础上我们希望能更大限度地提升下过滤的量级。我们把思路扩展到n叉树中，对于n叉树我们能得到**log**n**N**的指数模型，其中n代表每个节点的下级指针个数，假设n=10那么**log**10**N**=1000000就有N=5即O(5)的复杂度，这相对于O(20)又是4倍的I/O性能提升，关于n叉数我们设计了如下形态：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/8/4/165043c96db1ac69~tplv-t2oaga2asx-watermark.awebp)

每个节点将有四个指针，每个指针同样遵循上诉二叉树的设定规则：id值左边指针指向小于该id的下级范围，id值右边指针指向大于该id值的下级范围。整个n叉树我们直观上明显更“胖”了，意味着一次节点I/O后我们对目标数据能做出更大量级的细分！通过上文关于磁盘I/O的介绍，我们了解到磁盘以每个块作为最小操作单位，因此当前很多数据库产品在设计n叉树时有意地将一个块大小作为一个叶节点的大小，假设上诉叶节点不同元素占用情况为：左右指针各占4Byte，id值8Byte，目标记录指针4Byte，那么一个4Kb的磁盘块将大致可以容纳250个下级指针，100万行目标记录只需**log**250**N**=1000000即N=3的I/O次数，充分提升了每次节点I/O带来的检索效用！

因此，了解当前数据库表采用的索引构造，通过数学问题我们就可以很好的对实际项目中的一些查询做好成本分析，特别是大数据环境下，在预估查询时间时能很好地得出相应的性能表现。

当前不同数据库产品在设计索引时考虑了实际的应用场景，数据库表文件在磁盘的存放方式决定了需要采用怎样的索引结构。对于磁盘中按序存放的目标数据往往通过顺序索引就能实现快速检索，像一些大数据产品本身专注于查询性能，表数据往往只能进行数据追加而不支持删改操作，每次表数据的删改变动必须进行全局的格式化，这就是为了要保证数据在磁盘中的按序存放。而诸如mysql事务性数据库，针对的更多是常规业务操作，数据的增删改查是必须充分支持的，数据在磁盘的分布较为复杂，索引的设计上往往采用的是树形结构。

## 后语

关于索引的设计是一个较为复杂的过程，本文更多是希望在大脑中构造出数据查询中I/O成本分析的思考模型。索引极大的提升了数据库数据的查询速度，但我们也应该清楚的认识到，关于索引本身结构的维护也是一个不小的工程，不同索引结构本身的复杂性直接关系到索引结构本身维护过程中需要承担的I/O复杂度和CPU计算复杂度，关于索引的构建和维护可以另行查阅相关资料进行了解。

